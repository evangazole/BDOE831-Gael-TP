"""
Script PySpark pour lire les donn√©es depuis MinIO (S3).

Ce script configure Spark pour acc√©der √† MinIO et lit les fichiers CSV
en DataFrames pour afficher les sch√©mas et statistiques de base.
"""

from pyspark.sql import SparkSession

# Cr√©er la session Spark avec configuration MinIO
spark = SparkSession.builder \
    .appName("Read from MinIO") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .getOrCreate()

print("=" * 80)
print("Lecture des donn√©es depuis MinIO")
print("=" * 80)

# Lire Network_logs.csv
print("\nüìä Network_logs.csv")
print("-" * 80)
try:
    df_network = spark.read.csv(
        "s3a://data-lake/logs/Network_logs.csv",
        header=True,
        inferSchema=True
    )
    
    print(f"Nombre de lignes : {df_network.count()}")
    print("\nSch√©ma :")
    df_network.printSchema()
    print("\nAper√ßu des donn√©es :")
    df_network.show(5, truncate=False)
    print("\nStatistiques descriptives :")
    df_network.describe().show()
except Exception as e:
    print(f"‚ùå Erreur lors de la lecture : {e}")

# Lire Time-Series_Network_logs.csv
print("\nüìä Time-Series_Network_logs.csv")
print("-" * 80)
try:
    df_timeseries = spark.read.csv(
        "s3a://data-lake/logs/Time-Series_Network_logs.csv",
        header=True,
        inferSchema=True
    )
    
    print(f"Nombre de lignes : {df_timeseries.count()}")
    print("\nSch√©ma :")
    df_timeseries.printSchema()
    print("\nAper√ßu des donn√©es :")
    df_timeseries.show(5, truncate=False)
except Exception as e:
    print(f"‚ùå Erreur lors de la lecture : {e}")

# Lire dbip-country-lite
print("\nüìä dbip-country-lite-2026-01.csv")
print("-" * 80)
try:
    df_country = spark.read.csv(
        "s3a://data-lake/reference-data/dbip-country-lite-2026-01.csv",
        header=True,
        inferSchema=True
    )
    
    print(f"Nombre de lignes : {df_country.count()}")
    print("\nSch√©ma :")
    df_country.printSchema()
    print("\nAper√ßu des donn√©es :")
    df_country.show(5, truncate=False)
except Exception as e:
    print(f"‚ùå Erreur lors de la lecture : {e}")

# Lire client_hostname.csv
print("\nüìä client_hostname.csv")
print("-" * 80)
try:
    df_client = spark.read.csv(
        "s3a://data-lake/reference-data/client_hostname.csv",
        header=True,
        inferSchema=True
    )
    
    print(f"Nombre de lignes : {df_client.count()}")
    print("\nSch√©ma :")
    df_client.printSchema()
    print("\nAper√ßu des donn√©es :")
    df_client.show(5, truncate=False)
except Exception as e:
    print(f"‚ùå Erreur lors de la lecture : {e}")

print("\n" + "=" * 80)
print("‚úÖ Lecture termin√©e")
print("=" * 80)

spark.stop()
